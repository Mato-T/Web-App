# Introduction
- I created this web application called ReviewReview as an exercise for me to cope with all the data available on the internet. There is an unlimited amount of information for everyone, so it is important to be able to make sense of it. This application scrapes Amazon products, extracts relevant information and stores it in a database for faster access.
- The most important part of this application is retrieving the reviews of such products and offering certain filtering options. The user can filter by specific star ratings and even specify a keyword to be matched with the reviews.
- The target audience for this web app is the regular consumer who wants to make an informed buying decision. If durability of an iPhone case, for example, is an important factor in the purchase decision, the user can filter for reviews that mention durability in their rating. This website provides a much more convenient interface for "reviewing reviews" compared to the Amazon website.

# Data Requirements
- The main purpose of this application is to display reviews in an orderly fashion, meaning not much information is needed, either about the product or the reviews. For the product, I decided to include very basic information like the brand, the product name and the price.
- The most important information is the text of each review, as it contains all the information about the positive and negative aspects of the product. It also allows you to classify the review based on the label provided. I also included the number of stars the review has, as it gives a basic sense of the sentiment of that review.
- All this information is stored in a database. The reason for this is that scraping web pages takes a certain amount of time, which the user may not want to wait. Storing the results of the scraping in a database makes the next request for that product a simple database query. The simple database structure can be represented as follows:

  ![overview](https://user-images.githubusercontent.com/127037803/230575355-dab1e8d3-72b3-4e5b-8a2b-daeabf7d05d5.png) 
 - For more information, check out the SQL.txt file in this repository to view the SQL queries.
 
 # Features
- Bring the features of an interactive website across is quite a challenge, so I decided to create a short video. In a nutshell, the user enters a URL of an Amazon product into the text box, waits for the results to load, and then enters a label to filter the reviews displayed. For a more detailed look at the web page, see the features.mp4 file in this repository.

# Technologies
- In creating this application, I used several technologies. First, I hosted a database using MariaDB to store the information resulting from crawling the Amazon web pages. Of course, I also used the most common front-end technologies such as HTML, CSS, and JavaScript. Most of the web page is actually dynamically generated, meaning that the actual content of the web page is based on the queries to the database and not hard-coded into the page.
- As for the back-end technology, I set up a Flask app that connects to the database and initiates the crawling process. In addition, the Flask app forwards the text from the reviews to the Transformer model.
- As mentioned earlier, an important part of this application is the classification of the reviews. Since no labels are available, a zero-shot classifier is used. Zero-shot classifiers are usually trained with a pre-trained language model, such as BERT or GPT. The model uses its pre-trained knowledge of language and concepts to make a prediction about the class of input data by contrasting the input text and the label. For example, the label could be a prompt such as, "This text is about customer service."
- Hugging Face is a useful platform that provides a tremendous amount of different Transformer models. I chose to use the DeBERTa-v3-base model (https://huggingface.co/sileod/deberta-v3-base-tasksource-nli), which was fine-tuned on 520 tasks, including a number of datasets I considered relevant for my purpose (e.g., https://huggingface.co/datasets/app_reviews and https://huggingface.co/datasets/amazon_polarity).
- For scraping Amazon web pages, I used two main technologies. First, I used Selenium's Webdriver, an API that provides an interface to control the behavior of web browsers. A headless webdriver provides browser functionality without a visible user interface. The advantage of such a web browser is that it executes JavaScript and stores cookies to appear more human-like. This prevents Amazon from making CAPTCHA requests, for example.
- Using Selenium, I am able to retrieve the HTML code from the Amazon web page (though this takes more time than using the Requests library). I feel most comfortable using Beautiful Soup to parse HTML documents, so I used that library to extract the information I needed.

# Limitations
- This web application has several limitations. First, as shown in the demo video, it takes a long time to crawl the information from the Amazon website. Note that for demonstration purposes, I only crawled the first 10 reviews from each star (i.e. 10 one-star reviews, 10 two-star reviews, etc.). In many cases, there are thousands of reviews that can be crawled, which takes even more time.
- Secondly, I have tried to fine-tune this model even further, but have not succeeded. This is due to the limited computational resources I have available. In my case, I kept running out of CUDA memory because the model has millions of parameters. I tried truncating the text input, using a batch size of 1 with gradient accumulation, etc., but nothing seemed to work. Because of this, the model can be inaccurate from time to time. This is especially true if the text is 30 characters or less.
- Another major limitation is that this website does not work with all Amazon products. It works well with tangible items, such as hardware, household items, etc., but not with e-books, movies, or the like. This is because Amazon does not structure the web page of all its products the same way, and additional coding would be required for those products.
- Deep Learning is dominated by the English language, which means that the DeBERTa-v3-base was developed primarily for the English language. Therefore, I have decided to only accept products from the Amazon.com website.

# Conclusion
- This project was a great opportunity for me to combine different technologies into one meaningful project. The biggest challenge was scraping the information from the Amazon website due to inconsistent code structure and installed scraping traps. Ultimately, it was a tradeoff between speed and reliability.
- In addition, studying the HTML structure of established companies provided good insight into web coding practices. I've coded web applications before, but since the classification task is written entirely in Python, I had to spend more time learning back-end technologies other than just PHP. I feel much more comfortable using Python, so I'll be using Flask in future web projects (when appropriate).
- I'm a little disappointed that I wasn't able to fine-tune the model, as I think using a pre-trained model and then fine-tuning it to generalize it to the task at hand is a great way to create sophisticated models, even when limited resources are available. Important to note, however, is that there are smaller models available for fine-tuning, but nothing seemed to match the current one. In the future, I will use cloud-hosted notebooks and pre-trained models to achieve competitive results on more advanced NLP tasks.

